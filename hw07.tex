\documentclass[12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Math
\usepackage{fancyhdr} 
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros
\usepackage{calc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands and Custom Variables	
\newcommand{\problem}[1]{\hspace{-4 ex} \large \textbf{Problem #1} }
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\sint}{\text{s}\kern-5pt\int}
\newcommand{\powerset}{\mathcal{P}}
\renewenvironment{proof}{\hspace{-4 ex} \emph{Proof}:}{\qed}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\CC}{\mathbb{C}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%page
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
%\doublespacing
\allowdisplaybreaks
\pagestyle{fancy}
\fancyhf{}
\rhead{Shaw \space \thepage}
\setlength\parindent{0pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Code
\usepackage{listings}
\usepackage{courier}
\lstset{
	language=Python,
	showstringspaces=false,
	formfeed=newpage,
	tabsize=4,
	commentstyle=\itshape,
	basicstyle=\ttfamily,
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Images
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{float}

%tikz
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Hyperlinks
%\usepackage{hyperref}
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue,
%	filecolor=magenta,      
%	urlcolor=cyan,
%}

\begin{document}
	\thispagestyle{empty}
	
	\begin{flushright}
		Sage Shaw \\
		m565 - Fall 2017 \\
		\today
	\end{flushright}
	
{\large \textbf{HW 7}}\bigbreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{1 (a)} The following code computes the error using the Trapezoidal rule applied to the functions
\begin{align*}
	I(f_1) & = \int_{-1}^1 (x-1)^2e^{-x^2}dx \\
	I(f_2) & = \int_{-1}^1 e^{\cos(\pi x)} dx
\end{align*}

\begin{lstlisting}
def trap_int(f, a, b, n):
	h = (b-a)/n
	x = a + h
	T = 0
	for i in range(n-1):
	T += f(x)
	x += h
	T += (f(a)+f(b))/2
	T *= h
	return T

def f1(x):
	return (x-1)**2 * np.exp(-x**2)

def f2(x):
	return np.exp( np.cos( np.pi*x ) )

int1_actual = 1.87259295726583875
int2_actual = 2.53213175550401667

def p1a():
	ns = [4, 8, 16, 32]
	T1 = []
	T2 = []
	for n in ns:
		T1.append( abs( trap_int(f1, -1, 1, n) - int1_actual ) )
		T2.append( abs( trap_int(f2, -1, 1, n) - int2_actual ) )
	latex_table( (ns, T1, T2), 
('$n$', '$I(f_1)$ error', '$I(f_2)$ error') )
\end{lstlisting}

	The output is displayed in the table below.
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			$n$&$I(f_1)$ error&$I(f_2)$ error\\ \hline
			4&0.0312125372551&0.0109488793112\\ \hline
			8&0.0076967258198&3.98424961467e-07\\ \hline
			16&0.001918044355&4.4408920985e-16\\ \hline
			32&0.000479134582236&4.4408920985e-16\\ \hline
		\end{tabular}
	\end{center}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{1 (b)} The following code does the same for the corrected Trapezoidal rule:

	\begin{lstlisting}
def df1(x):
	return 2*(x-1)*np.exp(-x**2) - 2*x*(x-1)**2 * np.exp(-x**2)

def df2(x):
	return -np.pi*np.sin(np.pi*x)*np.exp(np.cos(np.pi*x))

def trap_int_corrected(f, df, a, b, n):
	T = trap_int(f,a,b,n)
	h = (b-a)/n 
	return T - h**2/12*(df(b)-df(a))

def p1b():
	int1_actual = 1.87259295726583875
	int2_actual = 2.53213175550401667
	ns = [4, 8, 16, 32]
	T1 = []
	T2 = []
	for n in ns:
		T1.append( abs( trap_int_corrected(f1, df1, -1, 1, n)
- int1_actual ) )
		T2.append( abs( trap_int_corrected(f2, df2, -1, 1, n)
- int2_actual ) )
	latex_table( (ns, T1, T2), 
('$n$', '$I(f_1)$ error', '$I(f_2)$ error') ) 
	\end{lstlisting}
	
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			$n$&$I(f_1)$ error&$I(f_2)$ error\\ \hline
			4&0.00055591715752&0.0109488793112\\ \hline
			8&3.25707953921e-05&3.98424961467e-07\\ \hline
			16&2.00559889851e-06&4.4408920985e-16\\ \hline
			32&1.24893210662e-07&4.4408920985e-16\\ \hline
		\end{tabular}
	\end{center}


\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{1 (c)} To verify that the error of the Trapezoidal rule decreases like $\mathcal{O}(h^2)$ We plot $h^2$ with the error. (More data points have been calcluated for this plot.)
	\begin{figure}[H]
		\caption{Error of the trapezoidal rule}
		\includegraphics[width=.75\textwidth]{hw7_p1_fig1}
		\label{p1_T_err}
		\centering
	\end{figure}
	Since the plot is roughly linear we can confirm that the error decreases like $\mathcal{O}(h^2)$. \bigbreak

	Similarly a plot of $h^4$ with the error of the corrected Trapezoidal rule shows that its error decreaces according to $\mathcal{O}(h^4)$.
	\begin{figure}[H]
		\caption{Error of the corrected trapezoidal rule}
		\includegraphics[width=.75\textwidth]{hw7_p1_fig2}
		\label{p1_T_corrected_err}
		\centering
	\end{figure}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{1 (d)} The rate of the error of the Trapezoidal rule applied to $I(f_2)$ will acually decreace along $\mathcal{O}(h^4)$. This is because $f_2^\prime(-1)=0= f_2^\prime(1)$ and thus the correction term for the corrected Trapezoidal rule is $0$. For the same justification that the error of the corrected trapezoidal rule decreaces by $\mathcal{O}(h^4)$ we can justify that the uncorrected Trapazoidal rule will too, since it has the same value.

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{2 (a)} Construct the cubic Hermite interpolating polynomial $p(x)$ on the interval $[a,b]$ such that
\begin{align*}
	p(a) &= f(a) & p^\prime(a) &= f^\prime(a) \\
	p(b) &= f(b) & p^\prime(b) &= f^\prime(b)
\end{align*}

	We can use Newton's divided difference method to derive the cubic Hermite interpolating polynomial.
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}\hline
			$x_i$ & $f[\cdot]$ & $f[\cdot,\cdot]$ & $f[\cdot,\cdot,\cdot]$ & $f[\cdot,\cdot,\cdot,\cdot]$\\ \hline
			$a$ & $f(a)$ & & & \\ \hline
			$a$ & $f(a)$ & $f^\prime(a)$ & & \\ \hline
			$b$ & $f(b)$ & $f[a,b]$ & $\frac{f[a,b] - f^\prime(a)}{h}$ & \\ \hline
			$b$ & $f(b)$ & $f^\prime(b)$ & $\frac{f^\prime(b) - f[a,b]}{h}$ & $\frac{f^\prime(b) - 2f[a,b] + f^\prime(a)}{h^2}$ \\ \hline
		\end{tabular}
	\end{center}
	$$p(x) = f(a) + (x-a)f^\prime(a) + (x-a)^2 \frac{f[a,b]-f^\prime(a)}{h} + (x-b)(x-a)^2 \frac{f^\prime(b) - 2f[a,b] + f^\prime(a)}{h^2}$$

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{2 (b)} Using part (a), derive a cubic Hermite quadrature formula. \bigbreak

	If $p(x)$ is the cubic Hermite interpolating polynomial of $f(x)$ at the points $a$ and $b$ then the quadrature is given by
	\begin{align*}
		\int_a^b f(x) & \approx \int_a^b p(x) \\
		& = \int_a^b f(a) + (x-a)f^\prime(a) + (x-a)^2 \frac{f[a,b]-f^\prime(a)}{h} \\
		& \phantom{=====} + (x-b)(x-a)^2 \frac{f^\prime(b) - 2f[a,b] + f^\prime(a)}{h^2} dx \\
		& = hf(a) + \tfrac{1}{2}h^2f^\prime(a) + \tfrac{1}{3}h^2 (f[a,b]-f^\prime(a)) - \tfrac{1}{12}h^2 (f^\prime(b) - 2f[a,b] + f^\prime(a)) \\
		& = \tfrac{1}{2}hf(a) + \tfrac{1}{2}hf(b) + \tfrac{5}{12}h^2f^\prime(a) - \tfrac{1}{12}h^2f^\prime(b)
	\end{align*}
	
\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{2 (c)} Give a formula for the error in part (a). \bigbreak 

	From class and the text we know that the error is given by
	$$
	\vert f(x) - p(x) \vert = \frac{(x-a)^2(x-b)^2}{4!}f^{(4)}(\xi)
	$$
	For some $\xi \in (a,b)$.
	
\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{2 (d)} Derive the error for the quadrature formula in part (b). \bigbreak

	\begin{align*}
		\left \vert \int_a^b f(x) dx - \int_a^b p(x) dx \right \vert &= \left \vert \int_a^b f(x) - p(x) dx \right \vert \\
		& = \left \vert \int_a^b \frac{(x-a)^2(x-b)^2}{4!}f^{(4)}(\xi) dx \right \vert \\
		& = \left \vert \frac{h^5}{720}f^{(4)}(\xi) \right \vert \\
		& \leq \max_{x \in (a,b)}\left \vert \frac{h^5}{720}f^{(4)}(x) \right \vert
	\end{align*}
	
\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{2 (e)} What is the degree of precision for the cubic Hermite quadrature formula? \bigbreak

	The degree of precision is 3, since the fourth derivative of cubic polynomials is always 0.
	
	
\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{3 (a)} Using the 5-node GQ formula in the provided Mathematica script, the nodes and weights for the integral $\int_0^1 f(x)\log(\tfrac{1}{x})dx$ have been calculated as follows\\
\begin{center}
	\begin{tabular}{|c|c|c|}\hline
		$i$ & $x_i$ & $w_i$\\ \hline
		0 & 0.029134472151 & 0.2978934717828945 \\ \hline
		1 & 0.173977213320 & 0.3497762265132242 \\ \hline
		2 & 0.411702520284 & 0.2344882900440524 \\ \hline
		3 & 0.677314174582 & 0.09893045951663315 \\ \hline
		4 & 0.894771361031 & 0.01891155214319580 \\ \hline
	\end{tabular}
\end{center}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{3 (b)} The code below uses the nodes and weights calculated above to determine $\int_0^1 \sin(x)\log(\tfrac{1}{x})dx \approx 0.239811742000565$. The output shows that the error is $7.71049890602\times 10^{-14}$. This is a very good approximation for using only samplings of the function.

\begin{lstlisting}
def p3b():
	xs = [0.02913447215197205,
		0.1739772133208976,
		0.4117025202849020,
		0.6773141745828204,
		0.8947713610310083]
	ws = [0.2978934717828945,
		0.3497762265132242,
		0.2344882900440524,
		0.09893045951663315,
		0.01891155214319580]
	correct = 0.239811742000565
	ys = np.sin(xs)
	GQ = np.dot(ws,ys)
	print("Gaussian Quadrature:")
	print(GQ)
	print("Error: ")
	print((GQ - correct))

**********************************
Output:
>>> p3b()
Gaussian Quadrature:
0.239811742001
Error: 
7.71049890602e-14
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{3 (c)} The nodes and weights for GQ applied to the function $\int_0^1 f(x)dx$ are below. After is a program using these weights to calculate the same integral as above. The error can be seen to be far less acurate: error$= 0.000285041178903$. Knowing the weight function in advance greatly improves the GQ algorithm.
\begin{center}
	\begin{tabular}{|c|c|c|}\hline
		$i$ & $x_i$ & $w_i$\\ \hline
		0 & 0.04691007703066800 & 0.1184634425280945 \\ \hline
		1 & 0.2307653449471585 & 0.2393143352496832 \\ \hline
		2 & 0.5 & 0.2844444444444444 \\ \hline
		3 & 0.7692346550528415 & 0.2393143352496832 \\ \hline
		4 & 0.9530899229693320 & 0.1184634425280945 \\ \hline	
	\end{tabular}
\end{center}
\begin{lstlisting}
def p3c():
	xs = [0.04691007703066800,
		0.2307653449471585,
		0.5000000000000000,
		0.7692346550528415,
		0.9530899229693320]
	ws = [0.1184634425280945,
		0.2393143352496832,
		0.2844444444444444,
		0.2393143352496832,
		0.1184634425280945]
	correct = 0.239811742000565
	ys = np.sin(xs)*np.log(1.0/np.array(xs))
	GQ = np.dot(ws,ys)
	print("Gaussian Quadrature:")
	print(GQ)
	print("Error: ")
	print((GQ - correct))

**********************************
Output:
>>> p3c()
Gaussian Quadrature:
0.240096783179
Error: 
0.000285041178903
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{3 (d)} Using 11 node GQ on $\int_0^1 f(x) dx$ we obtain the nodes and weights below. Usint them to evaluate the same integral as above the program below gives us the error: $1.44841479686\times 10^{-5}$. Knowing the weight function in advance is marvelous indeed!
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		$i$&$x_i$&$w_i$\\ \hline
		0&0.0108856709269715&0.02783428355808683\\ \hline
		1&0.05646870011595235&0.0627901847324523\\ \hline
		2&0.1349239972129753&0.09314510546386713\\ \hline
		3&0.2404519353965941&0.1165968822959952\\ \hline
		4&0.3652284220238275&0.1314022722551233\\ \hline
		5&0.5&0.1364625433889503\\ \hline
		6&0.6347715779761725&0.1314022722551233\\ \hline
		7&0.7595480646034058&0.1165968822959952\\ \hline
		8&0.8650760027870247&0.09314510546386713\\ \hline
		9&0.9435312998840476&0.0627901847324523\\ \hline
		10&0.9891143290730285&0.02783428355808683\\ \hline
	\end{tabular}
\end{center}
\begin{lstlisting}
def p3d():
	xs = [0.01088567092697150,
		0.05646870011595235,
		0.1349239972129753,
		0.2404519353965941,
		0.3652284220238275,
		0.5,
		0.6347715779761725,
		0.7595480646034059,
		0.8650760027870247,
		0.9435312998840476,
		0.9891143290730285]
	ws = [0.02783428355808683,
		0.06279018473245231,
		0.09314510546386713,
		0.1165968822959952,
		0.1314022722551233,
		0.1364625433889503,
		0.1314022722551233,
		0.1165968822959952,
		0.09314510546386713,
		0.06279018473245231,
		0.02783428355808683]
correct = 0.239811742000565
ys = np.sin(xs)*np.log(1.0/np.array(xs))
GQ = np.dot(ws,ys)
print("Gaussian Quadrature:")
print(GQ)
print("Error: %f")
print((GQ - correct))

**********************************
Output:
>>> p3d()
Gaussian Quadrature:
0.239826226149
Error: %f
1.44841479686e-05
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{4 (a)} Using Romberg integration we approximate the integral
$$
I(f_1) = \int_0^1 \frac{4}{1+x^2} dx = \pi
$$
by $R_{6,6} = 3.14159265364$.

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{4 (b)} The error in the Romberg approximations are shown below.

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		$n$&$R_{n,n}$&err\\ \hline
		1&3.0&-0.14159265359\\ \hline
		2&3.13333333333&-0.00825932025646\\ \hline
		4&3.14211764706&0.000524993469031\\ \hline
		8&3.14158578376&-6.86982791853e-06\\ \hline
		16&3.14159266528&1.1687923962e-08\\ \hline
		32&3.14159265364&4.84510209731e-11\\ \hline
	\end{tabular}
\end{center}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{4 (c)} Here is a listing of the code used for the calculation of part (a).

\begin{lstlisting}
def p4f(x):
	return 4/(1+x**2)

def p4a():
	N=5
	xs = np.linspace(0,1,2**N+1,endpoint=True)
	fs = p4f(xs)
	fs[0] *= .5
	fs[-1] *= .5
	
	Rom = np.zeros((N+1,N+1))
	for i in range(0,N+1):
		Rom[i][0] = np.sum(fs[::2**(N-i)])/2**i
	
	for m in range(1, N+1):
		for j in range(1, m+1): #j = k-1
			Rom[m,j] = (4**j * Rom[m,j-1] - 
Rom[m-1,j-1]) / (4**j - 1)
	ns = 2**np.arange(0,N+1)
	Rs = np.array([Rom[i][i] for i in range(0,N+1)])
	err = Rs - np.pi
	latex_table( (ns, Rs, err), ('$n$', '$R_{n,n}$', 'err') )
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{5 (a)} We are given the following integral
$$
I(f) = \int_0^1\int_0^1\int_0^1\int_0^1\int_0^1\int_0^1 \sin(\tfrac{\pi}{2}(x_1+x_2+x_3+x_4+x_5+x_6)) dx_1dx_2dx_3dx_4dx_5dx_6
$$
The following code uses SymPy (a symbolic math package for Python) and determines that $I(f) = \frac{-512}{\pi^6}$.

\begin{lstlisting}
def p5a():
	x = Symbol('x')
	y = Symbol('y')
	z = Symbol('z')
	w = Symbol('w')
	a = Symbol('a')
	b = Symbol('b')
	return integrate(sin(pi/2*(x+y+z+w+a+b)), (x,0,1),(y,0,1),
(z,0,1),(w,0,1),(a,0,1),(b,0,1))
********************************
Output:
>>> p5a()
-512/pi**6
\end{lstlisting}


\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{5 (b)} The following Python script computes a quasi-Monte Carlo approximation to the integral above and finds that $I(f) \approx -0.53900001389065677$ and the error is $0.0085708675600312256$.

\begin{lstlisting}
def p5b():
	N = 10000
	points = np.random.rand(N,6)
	col_sum = np.sum(points, axis=1)
	QMC = sum( np.sin(np.pi/2 * col_sum) ) / N
	return QMC
********************
Output
>>> p5b()
-0.53900001389065677
>>> p5b() +521/np.pi**6
0.0085708675600312256
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{5 (c)} I found a Python module called ghalton that is supposed to generate Halton Sequences, however it gave inconsistent results. Most of the time it was no better an approximation than the regular pseudorandom approach above, and ever once in a while it would give results comparable to MATLAB. I susspect that the module has an error, or that I misunderstood the documentation. Alas, the MATLAB code is given below. The error in the result using the Halton sequences is $-2.406470957178053e-04$.

\begin{lstlisting}
rng(11032016);
N = 10000;
points = net(haltonset(6),N);
col_sum = sum(points,2);
I = sum(sin(pi/2*col_sum))/N + 512/pi^6
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{5 (d)} The trapezoidal rule applied with $n=10$ points in each dimention gives an error of $0.006539223165815$. The code provided from the problem has been omitted for brevity. 

\begin{lstlisting}
n = 10;
[unmodified code given provided in the problem]
function [y] = f(x1,x2,x3,x4,x5,x6)
	y = sin(pi/2*(x1+x2+x3+x4+x5+x6));
end
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{5 (e)} Using the Trapezoidal rule is the worst approximation and used 100 times as many points. The Halton set points gave us a few more digits of accuracy than the regular pseudorandom points making the quasi-Monte carlo method using Halton set points over the domain the best of the three techniques in this instance. 
	
\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{6 (a)} See that
\begin{align*}
	1 + \sum_{k=2}^\infty \Big [ \tfrac{1}{k} + \log(1-\tfrac{1}{k}) \Big] & = 1 + \lim_{N \to \infty} \sum_{k=2}^N \Big [ \tfrac{1}{k} + \log(1-\tfrac{1}{k}) \Big] \\
	& = 1 + \lim_{n \to \infty} \sum_{k=2}^n \tfrac{1}{k} + \sum_{k=2}^n \log(1-\tfrac{1}{k}) \\
	& = \lim_{n \to \infty} \sum_{k=1}^n \tfrac{1}{k} + \sum_{k=2}^n \log(1-\tfrac{1}{k}) \\
	& = \lim_{n \to \infty} \left( \sum_{k=1}^n \tfrac{1}{k} \right) + \log \left( \prod_{k=2}^n 1-\tfrac{1}{k} \right) \\
	& = \lim_{n \to \infty} \left( \sum_{k=1}^n \tfrac{1}{k} \right) + \log \left( \prod_{k=2}^n \tfrac{k-1}{k} \right) \\
	& = \lim_{n \to \infty} \left( \sum_{k=1}^n \tfrac{1}{k} \right) + \log \left( \frac{(n-1)!}{n!} \right) \\
	& = \lim_{n \to \infty} \left( \sum_{k=1}^n \tfrac{1}{k} \right) + \log \left( n^{-1} \right) \\
	& = \lim_{n \to \infty} \left( \sum_{k=1}^n \tfrac{1}{k} \right) - \log \left( n \right) \\
	& = \gamma = 0.5772156649015328\dots
\end{align*}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{6 (b)} Using the Euler-MacLaurin formula after suming the first $k=2, \dots,19$ terms we can approximate $\gamma$. Let $f(x) = \frac{1}{x} + \log(1-\tfrac{1}{x})$. Then we use the following facts to do so
\begin{align*}
	\int_{20}^\infty f(x) dx & = 38\tanh^{-1}(\tfrac{1}{39}) -1 \\
	f^\prime(x) & = -x^{-2} - x^{-1} + (x-1)^{-1} \\
	f^{\prime\prime\prime}(x) & = -6x^{-4} - 2x^{-3} + 2(x-1)^{-3} \\
	f^{(5)}(x) & = -120x^{-6} - 24x^{-5} + 24(x-1)^{-5}
\end{align*}
This is done in the following program
\begin{lstlisting}
gamma = 0.5772156649015328

def p6_f(x):
	return 1/x + np.log(1-1/x)
	
def p6_f1(x):
	return -x**-2 + (x-1)**-1 - x**-1
	
def p6_f3(x):
	return -6*x**-4 + 2*(x-1)**-3 - 2*x**-3
	
def p6_f5(x):
	return -120*x**-6 + 24*(x-1)**-5 - 24*x**-5
	
def euler_maclaurin_terms(N):
	return p6_f(N)/2 - p6_f1(N)/12 + p6_f3(N)/720 - p6_f5(N)/30240

def gamma_approx():
	gamma_approx = 1
	for k in range(2,20):
		gamma_approx += 1/k + np.log(1-1/k)
		gamma_approx += 38*np.arctanh(1/39)-1 + euler_maclaurin_terms(20)
	return gamma_approx
\end{lstlisting}

When executed it approximates $\gamma \approx 0.577215664901$ for which the error is $3.74145159299 \times 10^{-14}$

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{6 (c)} When summed directly the first 1000 terms of the series give an error of $0.000498918995019$. This is far worse than using the Euler MacLaurin formula. This calculation is performed in the following program.

\begin{lstlisting}
def p6c():
	approx = 1
	for k in range(2,1003):
		approx += 1/k + np.log(1-1/k)
	print(approx - gamma)
\end{lstlisting}

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{7 (a)} Given the integral 
$$\int_0^{2\pi} e^{\cos(x)}dx \approx 7.95492652101284$$
we would like to estimate the error of the trapezoidal rule. First we define the coefficients of the Fourier series as follows
\begin{align*}
	a_0 & = \frac{1}{2\pi} \int_0^{2\pi} e^{\cos(x)}dx & a_n & = \frac{1}{\pi} \int_0^{2\pi} e^{\cos(x)}\cos(nx)dx
\end{align*}
Then $e^{\cos(x)} = \sum\limits_0^\infty a_n \cos(nx)$. Note that $2\pi a_0 = \int_0^{2\pi} e^{\cos(x)}dx$. We can then approximate it using the trapezoidal rule as follows
\begin{align*}
	\int_0^{2\pi} e^{\cos(x)}dx &= \int_{n=0}^{2\pi} \sum\limits_0^\infty a_n \cos(nx) dx \\
	& \approx \frac{2\pi}{N}\sideset{}{'}\sum_{k=0}^N \sum\limits_{n=0}^\infty a_n \cos(n\tfrac{2\pi}{N}k) \\
	& = \frac{2\pi}{N}\sideset{}{'}\sum_{k=0}^N a_0 + \sum\limits_{n=1}^\infty a_n \cos(n\tfrac{2\pi}{N}k) \\
	& = 2\pi a_0 + \frac{2\pi}{N}\sideset{}{'}\sum_{k=0}^N \sum\limits_{n=1}^\infty a_n \cos(n\tfrac{2\pi}{N}k) \\
\end{align*}
This last step follows since the trapezoidal rule is exact for constants. Then our error is given by
\begin{align*}
	\text{error} & \approx \frac{2\pi}{N}\sideset{}{'}\sum_{k=0}^N \sum\limits_{n=1}^\infty a_n \cos(n\tfrac{2\pi}{N}k) \\
	& = \frac{2\pi}{N} \sum_{k=0}^{N-1} \sum\limits_{n=1}^\infty a_n \cos(n\tfrac{2\pi}{N}k) \\
	& \text{\Big(since $\cos(n\tfrac{2\pi}{N}0) = \cos(n\tfrac{2\pi}{N}N)$ \Big)} \\
	& = \frac{2\pi}{N}  \sum\limits_{n=1}^\infty a_n \sum_{k=0}^{N-1} \cos(n\tfrac{2\pi}{N}k) \\
\end{align*}
From the identity
$$
\sum_{k=0}^{N-1} \cos(\omega k) = \cos \Big( \tfrac{1}{2}(N-1)\omega \Big)\csc(\tfrac{\omega}{2})\sin(\tfrac{N\omega}{2})
$$
we can see that our sum of cosines is zero except when $n$ is a multiple of $N$. In that case, the right hand side has indeterminant form, but we can easily evaluate the left hand side to be $N$. Thus
\begin{align*}
	\text{error} & \approx \frac{2\pi}{N}  \sum\limits_{n=1}^\infty a_n \sum_{k=0}^{N-1} \cos(n\tfrac{2\pi}{N}k) \\
	& = \frac{2\pi}{N}  \sum\limits_{z=1}^\infty a_{Nz} N \\
	& = 2\pi  \sum\limits_{z=1}^\infty a_{Nz}
\end{align*}
Thus the dominant error term is $2\pi a_N$

\bigbreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{7 (b)} To obtain an accuracy within a tolerance or $10^{-60}$ we want $2\pi a_N \approx 10^{-60}$. Since $\lim\limits_{n \to \infty}a_n 2^{n-2}n! = 1$ we would like $\frac{2\pi}{2^{n-2}n!} \leq 10^{-60}$. A short brute force search finds that we only need $m=41$ nodes before we are within a tollerance of $10^{-60}$.


\end{document}
